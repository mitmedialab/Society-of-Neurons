{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ExAe1UCWn5E"
      },
      "source": [
        "# Prompt to Activations\n",
        "\n",
        "This notebook contains minimal code for running an LLM using transformers and saving the outputs as a .pt file to your google drive. The file saves all the hidden states, but can be configured to also save the self-attention.\n",
        "\n",
        "Currently, the model being tested is [Alpaca-LoRA](https://github.com/tloen/alpaca-lora/), 7B parameters\n",
        "\n",
        "## Description of the saved file\n",
        "\n",
        "The input prompt, generated output and hidden states are saved as a `.pt` file from pytorch. \n",
        "\n",
        "The file is saved as `{input_prompt}.pt`\n",
        "\n",
        "To load the file use,\n",
        "\n",
        "`data = torch.load(\"{input_prompt}.pt\", map_location=torch.device('cpu'))`\n",
        "\n",
        "A peek into what that file looks like when loaded:\n",
        "```\n",
        "prompt = data['prompt']\n",
        "hidden_states = data['hidden_states']\n",
        "output_sequence = data['sequences'][0]\n",
        "output = data['output'].split(\"Response:\")[1]\n",
        "```\n",
        "\n",
        "The shape of the hidden states will be:\n",
        "\n",
        "```\n",
        "hidden states for full output shape: (n_output_tokens, n_layers, num_beams, n_iterations, hidden_size)\n",
        "\n",
        "n_output_tokens : includes the input tokens, I think even in input each token is fed one at a time\n",
        "n_layers : 33, number of decoder layers + input layer\n",
        "num_beams : 1, number of beam searches\n",
        "n_iterations: n_input_tokens, for first and then 1 for all other output tokens\n",
        "hidden_size: 4096, based on model config\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_pz8MuY84Qh"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q git+https://github.com/wazeerzulfikar/transformers.git\n",
        "!pip install -q git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive"
      ],
      "metadata": {
        "id": "HGUqO5sa-V6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!ls '/content/drive/MyDrive/llm'"
      ],
      "metadata": {
        "id": "r5XYt1pC8quP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the model"
      ],
      "metadata": {
        "id": "EdFzdIyh-bi1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VucO3HSMoJkz"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
        "\n",
        "model_path = '/content/drive/MyDrive/llm/models/llama-7b-hf'\n",
        "# model_path = 'decapoda-research/llama-7b-hf'\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained('decapoda-research/llama-7b-hf')\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(model, 'chainyo/alpaca-lora-7b')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility functions to run LLM"
      ],
      "metadata": {
        "id": "c8mB33o--dqv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3_lzwcqermJ"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(instruction):\n",
        "    return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "def evaluate(input_prompt, generation_config, output_hidden_states=True):\n",
        "    '''\n",
        "    Takes the instruction, puts it in the instruction finetuning template and returns the model generated output, along with the hidden states\n",
        "    '''\n",
        "    \n",
        "    prompt = generate_prompt(input_prompt)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256,\n",
        "        output_hidden_states=output_hidden_states\n",
        "    )\n",
        "\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s)\n",
        "        print(\"Response:\", output.split(\"### Response:\")[1].strip())\n",
        "\n",
        "    return generation_output\n",
        "\n",
        "def save_output(input_prompt, generation_output, save_path):\n",
        "    '''\n",
        "    Saves the generation output as a whole as a pytorch file.\n",
        "    '''\n",
        "    output_to_save = generation_output\n",
        "    output_to_save['prompt'] = input_prompt\n",
        "\n",
        "    for s in generation_output.sequences:\n",
        "        output_tokens = tokenizer.decode(s)\n",
        "\n",
        "    output_to_save['output'] = output_tokens\n",
        "\n",
        "    torch.save(output_to_save, save_path)\n",
        "    print(\"Saved to\", save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the LLM and save the output"
      ],
      "metadata": {
        "id": "z7CYn01B_NCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the input prompt\n",
        "generation_config = GenerationConfig(\n",
        "      temperature=0,\n",
        "      top_p=1,\n",
        "      num_beams=1, # beam search\n",
        "    )\n",
        "\n",
        "input_prompt = \"What is 5+9?\"\n",
        "save_path = \"/content/drive/MyDrive/llm/activations/{}.pt\".format(input_prompt.replace(' ', '_'))\n",
        "\n",
        "generation_output = evaluate(input_prompt, generation_config)\n",
        "save_output(input_prompt, generation_output, save_path)"
      ],
      "metadata": {
        "id": "tzXrY3RcN6rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5ASOjgeb_7DO"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}