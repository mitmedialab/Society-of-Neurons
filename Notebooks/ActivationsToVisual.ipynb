{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ExAe1UCWn5E"
      },
      "source": [
        "# Activations to Visualization\n",
        "\n",
        "This notebook contains code for loading and visualizing activations of an LLM during output generation. The code assumes the generated output is stored in a saved .pt file on your google drive. \n",
        "\n",
        "\n",
        "Currently, the model being tested is [Alpaca-LoRA](https://github.com/tloen/alpaca-lora/), 7B parameters\n",
        "\n",
        "## Description of the saved file\n",
        "\n",
        "The input prompt, generated output and hidden states are saved as a `.pt` file from pytorch. \n",
        "\n",
        "The file is saved as `{input_prompt}.pt`\n",
        "\n",
        "To load the file use,\n",
        "\n",
        "`data = torch.load(\"{input_prompt}.pt\", map_location=torch.device('cpu'))`\n",
        "\n",
        "A peek into what that file looks like when loaded:\n",
        "```\n",
        "prompt = data['prompt']\n",
        "hidden_states = data['hidden_states']\n",
        "output_sequence = data['sequences'][0]\n",
        "output = data['output'].split(\"Response:\")[1]\n",
        "```\n",
        "\n",
        "The shape of the hidden states will be:\n",
        "\n",
        "```\n",
        "hidden states for full output shape: (n_output_tokens, n_layers, num_beams, n_iterations, hidden_size)\n",
        "\n",
        "n_output_tokens : includes the input tokens, I think even in input each token is fed one at a time\n",
        "n_layers : 33, number of decoder layers + input layer\n",
        "num_beams : 1, number of beam searches\n",
        "n_iterations: n_input_tokens, for first and then 1 for all other output tokens\n",
        "hidden_size: 4096, based on model config\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGUqO5sa-V6V"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r5XYt1pC8quP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9fa7d4e-782e-425d-8f4a-cdbf6b800eca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "activations  models\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!ls '/content/drive/MyDrive/llm'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import Packages"
      ],
      "metadata": {
        "id": "5TfmsoUfmLPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.colors import LogNorm\n",
        "import torch\n",
        "from tqdm import tqdm "
      ],
      "metadata": {
        "id": "xpsgG7onhFTw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdFzdIyh-bi1"
      },
      "source": [
        "### Load Activations (.pt file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_prompt = \"What_is_4_+_2?\"\n",
        "\n",
        "activation_path = \"/content/drive/MyDrive/llm/activations/\"+str(input_prompt)+\".pt\"\n",
        "data = torch.load(activation_path, map_location=torch.device('cpu'))\n",
        "\n",
        "# from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
        "# tokenizer = LlamaTokenizer.from_pretrained('decapoda-research/llama-7b-hf')"
      ],
      "metadata": {
        "id": "DPgYNXBF4DIH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define Visualization Options & Utility Functions\n"
      ],
      "metadata": {
        "id": "E4vypa5ilUpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bitmap_animation(data):\n",
        "\n",
        "  hidden_states = data['hidden_states']\n",
        "  output_response = data['output'].split(\"Response:\")[1]\n",
        "  # tokenized_output_response = tokenizer.encode(output_response)\n",
        "\n",
        "  all_images = []\n",
        "  vmin=0\n",
        "  vmax=0\n",
        "\n",
        "  #iterate through hidden states of all tokens \n",
        "  for token_id, token_hidden_states in tqdm(enumerate(hidden_states)):\n",
        "    # current_token = tokenized_output_response[token_id]\n",
        "    current_token_string = output_response[token_id]\n",
        "    # print(\"Token ID: \"+str(token_id)+\"\\t Token Value: \" + str(current_token), \"\\t Token String: '\" + str(current_token_string)+\"'\")\n",
        "\n",
        "    # Initialize an empty dictionary to store activations\n",
        "    activations = []\n",
        "\n",
        "    # iterate through all layers for each token's hidden states\n",
        "    for layer_id, layers in enumerate(token_hidden_states):\n",
        "      # print(\"Layer: \"+str(layer_id))\n",
        "      for beam_id, beams in enumerate(layers):\n",
        "        # print(\"Beam: \"+str(beam_id))\n",
        "        for token_activation_id, token_activations in enumerate(beams):\n",
        "          token_activations_np = token_activations.numpy()  # Detach and convert to NumPy array\n",
        "          activations.extend(token_activations_np)\n",
        "\n",
        "    # Determine the size of the square image\n",
        "    image_size = int(np.ceil(np.sqrt(len(activations))))\n",
        "\n",
        "    # Create an empty square image with pixel values set to zero\n",
        "    img = np.zeros((image_size, image_size), dtype=np.uint8)\n",
        "\n",
        "    # Fill in the image with the normalized activation values\n",
        "    img.flat[:len(activations)] = activations\n",
        "\n",
        "    # Add the image to the all_images list\n",
        "    all_images.append(img)\n",
        "\n",
        "    # Update the global vmin and vmax\n",
        "    vmin_token = np.min(activations)\n",
        "    if vmin_token < vmin:\n",
        "      vmin = vmin_token\n",
        "    vmax_token = np.max(activations)\n",
        "    if vmax_token > vmax:\n",
        "      vmax = vmax_token\n",
        "\n",
        "  #Set log norm based on global max and min activations values\n",
        "  log_norm = LogNorm(vmin=vmin, vmax=vmax)\n",
        "\n",
        "  # Set the matplotlib backend to save files in the desired format\n",
        "  matplotlib.use(\"Agg\")\n",
        "\n",
        "  # Define the update function for the animation\n",
        "  def update(frame):\n",
        "      plt.clf()\n",
        "      plt.imshow(all_images[frame], cmap='viridis', norm=log_norm)\n",
        "      plt.title(f\"Token {frame + 1}: {output_response[frame]}\", fontsize=30)\n",
        "      plt.axis('off')\n",
        "\n",
        "  # Create the animation\n",
        "  fig = plt.figure(figsize=(20, 20))\n",
        "  ani = FuncAnimation(fig, update, frames=len(output_response), interval=250)\n",
        "\n",
        "  # Save the animation as an MP4 file\n",
        "  output_file = data[\"prompt\"].replace(' ', '_')+\".mp4\"\n",
        "  output_path = \"/content/drive/MyDrive/llm/visualizations/\"+str(output_file)\n",
        "  ani.save(output_file, dpi=100, writer=\"ffmpeg\")\n",
        "\n",
        "  # Set the matplotlib backend back to the default\n",
        "  matplotlib.use(\"module://ipykernel.pylab.backend_inline\")\n",
        "\n",
        "  print(\"visualization succesfully saved: '\"+str(output_path)+\"'\")\n",
        "\n",
        "def generate_visuals(data, visual_type=\"bitmap_animation\"):\n",
        "  if visual_type == \"bitmap_animation\":\n",
        "    generate_bitmap_animation(data)"
      ],
      "metadata": {
        "id": "adsI_rpAO3Cp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run Visual Generation & Save Ouput"
      ],
      "metadata": {
        "id": "W0BYWgiOksy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_visuals(data, visual_type=\"bitmap_animation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L35qD3yi8qi",
        "outputId": "deff1f76-2242-43ca-d756-bd7c9416dee1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10it [00:02,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "visualization succesfully saved: '/content/drive/MyDrive/llm/visualizations/What_is_4_+_2?.mp4'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}